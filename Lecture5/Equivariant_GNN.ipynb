{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, symmetries are usually described by $groups$.\n",
    "We can characterize the relationship between a function (such as a neural network layer) and a symmetry group by considering its \\textit{equivariance} properties.\n",
    "A map $f: X \\rightarrow Y$ is said to be equivariant w.r.t. the actions $\\rho:G\\times X\\to X$ and $\\rho':G\\times Y\\to Y$ of a group $G$ on $X$ and $Y$ if\n",
    "$$\n",
    "    f\\Big(\\rho_g(x) \\Big) = \\rho_g' \\Big(f(x)\\Big)\\,\n",
    "$$\n",
    "\n",
    "Reference : arXiv 2203.06153\n",
    "\n",
    "<center width=\"600%\"><img src=\"invariance_vs_equivariance.png\" alt=\"Alternative text\"  width=\"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The equivariant transformation on the graph is defined by \n",
    "$$\\phi \\Big( T_g (x) \\Big) = S_g  \\Big( \\phi (x)\\Big) $$\n",
    "<center width=\"500%\"><img src=\"egnn.png\" alt=\"Alternative text\"  width=\"500px\"></center>\n",
    "\n",
    "The equivariant graph convolutional layer EGCL (ref : 2102.09844) is defined by as following \n",
    "\n",
    "$$h^{l+1}, x^{l+1} = EGCL(h^l, x^l, \\mathcal{E})$$\n",
    "\n",
    "It happens over following steps\n",
    "\n",
    "$$m_{ij} = \\phi_e \\Bigg( h_i^l, h_j^l, ||x_i^l - x_j^l||^2, a_{ij}\\Bigg)$$\n",
    "$$x_i^{l+1} = x_i^l + C \\sum_{j \\in \\mathcal{N}_i} (x_i^l - x_j^l) ~\\phi_x (m_{ij})$$\n",
    "$$m_i = \\sum_{j \\in \\mathcal{N}_i} m_{ij}$$\n",
    "$$h_i^{l+1} = \\phi_h(h_i^l, m_i)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/RobDHess/Steerable-E3-GNN\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, SiLU, Sequential\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_scatter import scatter\n",
    "\n",
    "\n",
    "class EGNNLayer(MessagePassing):\n",
    "    \"\"\"E(n) Equivariant GNN Layer\n",
    "\n",
    "    Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, edge_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim + 1 + edge_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = Sequential(\n",
    "            Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index, edge_attribute):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos, edge_attribute=edge_attribute)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j, edge_attribute):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists, edge_attribute], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)\n",
    "        # NOTE: some papers divide pos_diff by (dists + 1) to stabilise model.\n",
    "        # NOTE: lucidrains clamps pos_diff between some [-n, +n], also for stability.\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"sum\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lorentz equivariant GNN\n",
    "Reference : 2201.08187\n",
    "Proposition : A continuous function \n",
    "\n",
    "<center width=\"700%\"><img src=\"architecture.jpg\" alt=\"Alternative text\"  width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h^{l+1}, x^{l+1} = LGCL(h^l, x^l, \\mathcal{E})$$\n",
    "\n",
    "It happens over following steps\n",
    "\n",
    "$$m_{ij} = \\phi_e \\Bigg( h_i^l, h_j^l, \\psi(||x_i^l - x_j^l||^2), \\psi( \\langle x_i^l, x_j^l\\rangle)\\Bigg)$$\n",
    "$$w_{ij} = \\phi_{m}(m_{ij})$$\n",
    "$$x_i^{l+1} = x_i^l + C \\sum_{j \\in \\mathcal{N}_i}  \\phi_x (m_{ij})~x_j^l $$\n",
    "$$h_i^{l+1} = h_i^l + \\phi_h(h_i^l, \\sum_{j \\in \\mathcal{N}_i} w_{ij} m_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "local = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JetDataset import Jet_Dataset\n",
    "from mlp import build_mlp\n",
    "\n",
    "dataset_path = '/Users/sanmay/Documents/ICTS_SCHOOL/Main_School/JetDataset/'\n",
    "file_name = dataset_path + 'JetClass_example_100k.root' # -- from -- \"https://hqu.web.cern.ch/datasets/JetClass/example/\" #\n",
    "jet_dataset = Jet_Dataset(dataset_path=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski = torch.from_numpy(\n",
    "            np.array(\n",
    "                [\n",
    "                    [1.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, -1.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, -1.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, -1.0],\n",
    "                ],\n",
    "                #dtype=np.float32,\n",
    "            ))\n",
    "\n",
    "def innerprod(x1, x2):\n",
    "        return torch.sum(\n",
    "            torch.matmul(x2.T, torch.matmul(minkowski, x1)), dim=1, keepdim=True\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lorentz_GNNLayer(MessagePassing):\n",
    "    \"\"\"E(n) Equivariant GNN Layer\n",
    "\n",
    "    Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, coord_dim,  activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super(Lorentz_GNNLayer, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.coord_dim = coord_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_phi_e = build_mlp(2*emb_dim + 2*coord_dim, 1, features=[3, 4, 2])\n",
    "        self.mlp_phi_x = build_mlp(1, 1, features=[3, 4, 2])\n",
    "        self.mlp_phi_h = build_mlp(emb_dim+1, 1, features=[3, 4, 2])\n",
    "        self.mlp_phi_m = build_mlp(1, 1, features=[3, 4, 2])\n",
    "        \n",
    "        \n",
    "        self.minkowski = torch.from_numpy(\n",
    "            np.array(\n",
    "                [\n",
    "                    [1.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, -1.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, -1.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, -1.0],\n",
    "                ],\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def psi(self, x):\n",
    "        return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "    def innerprod(self, x1, x2):\n",
    "        return torch.sum(\n",
    "            torch.matmul(x2.T, torch.matmul(self.minkowski, x1)), dim=1, keepdim=True\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index, edge_attribute):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos, edge_attribute=edge_attribute)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, x_i, x_j):\n",
    "        # Compute messages\n",
    "        msg = torch.cat(\n",
    "            [h_i, h_j,\n",
    "                self.psi(self.innerprod(x_i - x_j, x_i - x_j)),\n",
    "                self.psi(self.innerprod(x_i, x_j))\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        \n",
    "        phi_x = self.mlp_phi_x(msg) * x_j\n",
    "        \n",
    "        w_ij = self.mlp_phi_m(msg) * msg\n",
    "        return phi_x, w_ij\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        phi_x, w_ij = inputs\n",
    "        # Aggregate messages\n",
    "        x_aggr = scatter(phi_x, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        w_aggr = scatter(w_ij, index, dim=self.node_dim, reduce=\"sum\")\n",
    "        return x_aggr, w_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, x):\n",
    "        x_aggr, w_aggr = aggr_out\n",
    "        upd_x = x + x_aggr\n",
    "        upd_h = h + w_aggr\n",
    "        return upd_x, upd_h\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = torch.randn(4, 1), torch.randn(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3278]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innerprod(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3714],\n",
       "        [-0.2663],\n",
       "        [-0.7747],\n",
       "        [ 0.0447]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7197]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x2.T, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse, to_undirected\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, knn_graph\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_scatter import scatter\n",
    "from torch_cluster import knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=jet_dataset, batch_size=5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_b = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, edge_index, batch = gr_b.x, gr_b.edge_index, gr_b.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([172, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
